{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run with prompt: ./duong/init_prompt_angelic_4.txt\n",
      "run with llm: deepseek-r1:70b\n",
      "======== Generate Angelic structure and Query Entailment: ========\n",
      " <think>\n",
      "Okay, so I'm trying to figure out whether the rescuer who pushed someone out of the way, causing their luxury kimono to get dirty, has to compensate for the damage. Let's break this down step by step using the legal framework provided.\n",
      "\n",
      "First, looking at Article 698, it talks about a manager intervening benevolently in another's business to save the principal from imminent danger. The key point here is that the manager isn't liable unless they acted in bad faith or with gross negligence. \n",
      "\n",
      "In the query scenario, the rescuer pushed someone out of harm's way, which prevented them from being hit by a car. This action saved the person from physical harm but resulted in damage to their kimono.\n",
      "\n",
      "So, applying Article 698, the root node R1 is \"The manager is not liable for damages resulting from benevolent intervention.\" Now, I need to identify abstract factors that support or deny this conclusion. \n",
      "\n",
      "Abstract Factor A1 would be whether the rescuer acted in bad faith or with gross negligence because if they did, liability would apply. The opposite of that would be acting in good faith and without gross negligence, which supports non-liability.\n",
      "\n",
      "Next, refining A1 into base-level factors: B1 could be \"The rescuer acted in good faith,\" and B2 could be \"The rescuer did not act with gross negligence.\" Both of these support the conclusion that there's no liability.\n",
      "\n",
      "In the query, the rescuer's action was clearly to prevent harm. There's no indication of bad faith or negligence; it seems like a good Samaritan act. Therefore, the situation matches B1 and B2 because the rescuer didn't act in bad faith or with gross negligence.\n",
      "\n",
      "So, putting it all together: The query entails B1 (good faith) and B2 (no gross negligence), which in turn support A1, leading to R1 that there's no liability for the damages. Therefore, the rescuer doesn't have to compensate for the kimono.\n",
      "</think>\n",
      "\n",
      "The scenario involves a rescuer who acted in good faith without gross negligence, thereby fitting base-level factors B1 and B2.\n",
      "\n",
      "**Step-by-Step Explanation:**\n",
      "\n",
      "1. **Root Node (R):** The manager is not liable unless acting in bad faith or with gross negligence.\n",
      "\n",
      "2. **Abstract Factor (A1):** The absence of bad faith or gross negligence supports non-liability.\n",
      "\n",
      "3. **Base-Level Factors (B1 & B2):**\n",
      "   - B1: Acting in good faith.\n",
      "   - B2: No gross negligence.\n",
      "\n",
      "4. **Links:**\n",
      "   - B1 supports A1.\n",
      "   - B2 supports A1.\n",
      "   - A1 supports R.\n",
      "\n",
      "5. **Query Application:** The rescuer's actions align with B1 and B2, thus supporting non-liability under Article 698.\n",
      "\n",
      "**Answer:** (query entails B1)  \n",
      "(query entails B2)\n",
      "\n",
      "The rescuer does not have to compensate for the kimono as they acted in good faith without gross negligence.\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# from duong.gen_angelic_st import GenAngelic\n",
    "from minh.deepseek import chat\n",
    "\n",
    "\n",
    "\n",
    "# chat = GenAngelic().init()\n",
    "class Answerer:\n",
    "    def __init__(self, debug=False):\n",
    "        self.debug = debug\n",
    "\n",
    "    def answer(self, article, query):\n",
    "        reply = chat(article, query)\n",
    "        if self.debug:\n",
    "            print(\"======== Generate Angelic structure and Query Entailment: ========\\n\", reply)\n",
    "\n",
    "text = \"\"\n",
    "with open(\"./test.txt\", \"r\") as f:\n",
    "    text = \"\".join(f.readlines())\n",
    "answerer = Answerer(debug=True)\n",
    "answer = answerer.answer(text, \"In cases where an individual rescues another person from getting hit by a car by pushing that person out of the way, causing the person's luxury kimono to get dirty, the rescuer does not have to compensate damages for the kimono.\")\n",
    "print(answer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/anguyen/miniconda3/envs/coliee2025/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======= Fact Extractor step ========\n",
      "\n",
      "Facts:\n",
      " entail(query, node(bfactor(1))).entail(query, node(bfactor(2))).link(support(node(bfactor(1)),node(afactor(1)))).link(support(node(bfactor(2)),node(afactor(1)))).link(support(node(afactor(1)),node(root(1)))).\n",
      "\n",
      "\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "from extractor.extractor import MDAExtractor\n",
    "# from duong.gen_angelic_st import GenAngelic\n",
    "from reasoner.reasoner import Reasoner\n",
    "import dspy\n",
    "\n",
    "# chat = GenAngelic().init()\n",
    "class Answerer:\n",
    "    def __init__(self, reply, debug=False):\n",
    "        self.extractor = MDAExtractor()\n",
    "        self.reasoner = Reasoner()\n",
    "        self.debug = debug\n",
    "        self.reply = reply\n",
    "\n",
    "    def answer(self, article, query):\n",
    "        facts = self.extractor.forward(text=self.reply)\n",
    "        if self.debug:\n",
    "            print(\"======= Fact Extractor step ========\\n\")\n",
    "            print(\"Facts:\\n\", facts)\n",
    "            print(\"\\n\")\n",
    "        # theory_2\n",
    "        return self.reasoner.reason(facts)\n",
    "\n",
    "\n",
    "\n",
    "dspy.settings.configure(\n",
    "    lm=dspy.LM(\n",
    "        model=\"ollama_chat/llama3.3\",  # Changed to include provider prefix\n",
    "        api_base=\"http://localhost:11435\",\n",
    "        max_tokens=20000,\n",
    "    )\n",
    ")\n",
    "text = \"\"\n",
    "reply = \"\"\n",
    "with open(\"./test_reply.txt\", \"r\") as f:\n",
    "    reply = \"\".join(f.readlines())\n",
    "answerer = Answerer(reply=reply, debug=True)\n",
    "\n",
    "answer = answerer.answer(text, \"In cases where an individual rescues another person from getting hit by a car by pushing that person out of the way, causing the person's luxury kimono to get dirty, the rescuer does not have to compensate damages for the kimono.\")\n",
    "print(answer)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "coliee2025",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
